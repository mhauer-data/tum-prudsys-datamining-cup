{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of I. Simple Preprocessing.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"EkjlTkz99c_Y","colab_type":"text"},"source":["# Simple preprocessing\n","\n","\n","Hello People! \n","\n","Welcome to the first Notebook. \n","\n","Like we described in our presentation. This is notebook is used for the creation of easy features. \n","\n","__What is an easy feature?__\n","\n","You can think of it like a column in one of the data files. At the beginning\n","we have the three input files (items, info, orders). In the end we will export 2 files: \n","- ```dynamic_features.pkl```-> All time dependent data such as date features (e.g day_of_month)\n","- ```static_features.pkl```-> All static features such as itemID and so on\n","\n","After this notebook we **only** work with ```dynamic_features.pkl``` and ```static_features.pkl```.\n","\n","_Why the fuck ```*.pkl```?!_ \n","\n","pkl in this case stands for 'pickle' and allows us to very space efficiently safe the data in bytes. When loading the data we don't have the problem with delimiters like we would have with csv. But most important: Later we need to save models and so on with pickle. So we already start here using it to have some consistency in these unconsistent times :D\n","\n","__How do we handle our test set ?__\n","As we already tried out in team 1, it is crucial to think about the complexity of test features beforehand. Therefore, we now prepare the test set from right the beginning. \n","\n","This means that we now already created rows for the testset. This means the aforementioned files then consist out of **all** the data (test + validation + train). _But why?!_ In the first two notebooks we have function (later more about those) to mutate our data. These mutation then directly can be done for test and train set, using the same functions. This finally leads to correct function. Furthermore, everyone who is adding a new feature now already needs to think about, how it may be applied to the train set (in which we have not all information)\n","\n","How the creation of testset features works, will be elaborated later. \n","\n","__What is the structure of this notebook?__\n","\n","1. **Setting up Colab**\n","1. **Notes and Ideas**\n","1. **Setting up Notebook**\n","1. **Create Dynamic_Features Dataframe** \n","1. **Deviation of Promotions and Creation of Test Set**\n","1. **Defining Feature Functions**\n","1. **Applying Features**\n","1. **Testset Mappings**\n","1. **Saving data**\n","\n","__TL;DR__\n","- Input = info.csv/ items.csv / orders.csv\n","- Output = dynamic_features.csv / static_features.csv\n","- Notebook for easy features (new Columns in source data)\n","- Test set is simulated and created already here.\n"]},{"cell_type":"markdown","metadata":{"id":"P94sbukY70Ka","colab_type":"text"},"source":["# 1. Setting up Colab\n","\n","Here we set up colab. U knooow it :D \n","\n","In case u don't use colab, please specify the variable ```use_colab``` to false. In case u don't use it its unavoidable to make sure that noone changed something online while u change it offline. The results of this would be worse than a division by zero.  "]},{"cell_type":"code","metadata":{"id":"9OwEnQ9q90xk","colab_type":"code","colab":{}},"source":["use_colab: bool = True"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-aKdIiCP-DAh","colab_type":"text"},"source":["In case u accidentall run the following code twice u will get following weird and confusing error:\n","\n","```\n","OSError: [Errno 107] Transport endpoint is not connected\n","```\n","\n","In this case just restart the runtime above. \n"]},{"cell_type":"code","metadata":{"id":"NW-V0LwRFBrz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":50},"executionInfo":{"status":"ok","timestamp":1592835514923,"user_tz":-120,"elapsed":4140,"user":{"displayName":"Kyro Aiad","photoUrl":"","userId":"01120278881651803707"}},"outputId":"8940886d-1ebc-4687-e2a5-4b1408ae5d51"},"source":["if use_colab:\n","  import os\n","  from google.colab import drive \n","  drive.mount(\"/content/gdrive\", force_remount=True)\n","  # Change directory for nicer imports\n","  %cd \"/content/gdrive/My Drive/Data_Mining_Cup/05 Code/\""],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n","/content/gdrive/My Drive/Data_Mining_Cup/05 Code\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"o0G7khZfI-7B","colab_type":"text"},"source":["# 2. Nodes and Ideas"]},{"cell_type":"markdown","metadata":{"id":"FhkpJPxiJBhP","colab_type":"text"},"source":["## Ideas for new Features\n","Please specify assignment in bold and brackets behind. If nobody is assigned put the \"To Be Assigned (TBA)\" flag. \n","\n","- Boolean two days after promotion (Nic)\n","- how rare are solds **(TBA)**\n","- Sum of solds 14 days after -> Can be used for the direct prediction of sum of values **(TBA)**\n"]},{"cell_type":"markdown","metadata":{"id":"xuI6qXPWJehT","colab_type":"text"},"source":["## Notes\n","\n","Currently no notes :( "]},{"cell_type":"markdown","metadata":{"id":"qg-T8Iok8z_X","colab_type":"text"},"source":["# 3. Setting up Notebook\n","\n","Here we setup some parameters for the notebook to work in the expected ways."]},{"cell_type":"markdown","metadata":{"id":"8LRU5vmI9eCt","colab_type":"text"},"source":["If we want to **use the final test phase** for the later submission, we need to specify the following parameter **```make_final_submission``` to true.** Otherwise the selfmade test set will be used."]},{"cell_type":"code","metadata":{"id":"LDWM8LmV_KkE","colab_type":"code","colab":{}},"source":["make_final_submission: bool = False"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3JVVjiq__R7I","colab_type":"text"},"source":["In case we don't want to make a final submission, we now have to specify the test_set. In case you change it, please leave a comment on why and what the new one is. If the end is defined as None we use all the data from start_period (inclusive) till the end of dataset. \n","\n","In case make_final_submission is True we do not use those parameters."]},{"cell_type":"code","metadata":{"id":"KvKp58Ug_QwD","colab_type":"code","colab":{}},"source":["# This will set the start of test period to 2 weeks before end of data\n","test_period_start: str=\"2018-06-02\"\n","# Use till the end\n","test_period_end: str=\"2018-06-15\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ThuLP_PksLxg","colab_type":"code","colab":{}},"source":["# These datapoints are for the final submision\n","final_submission_start: str=\"2018-06-30\"\n","final_submission_end: str=\"2018-07-13\"  # shouldn't it be 07-13 if inclusive? "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P01Jd3yw9GXT","colab_type":"text"},"source":["#### Imports\n","\n","Here you can add imports u need. Please use alphabetical order to not import new stuff twice."]},{"cell_type":"code","metadata":{"id":"vWcDzESD8HZ6","colab_type":"code","colab":{}},"source":["from datetime import datetime, timedelta\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","from typing import List, Callable\n","\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qynB18E7ASmI","colab_type":"text"},"source":["### Data Imports\n","\n","Now we import the source data. In case we use our test set, we delete the test set values to simulate later submission."]},{"cell_type":"code","metadata":{"id":"s1sqbn-Oq1QT","colab_type":"code","colab":{}},"source":["# In tables 'infos' and 'items' itemID is used as index\n","info = pd.read_csv('/content/gdrive/My Drive/Data_Mining_Cup/02 Data/DMC20_Data/infos.csv', delimiter='|', index_col=\"itemID\")\n","items = pd.read_csv('/content/gdrive/My Drive/Data_Mining_Cup/02 Data/DMC20_Data/items.csv', delimiter='|', index_col=\"itemID\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fqAFXUn5AiHR","colab_type":"code","colab":{}},"source":["orders = pd.read_csv('/content/gdrive/My Drive/Data_Mining_Cup/02 Data/DMC20_Data/orders.csv', delimiter='|', parse_dates=True)\n","orders['time'] = pd.to_datetime(orders['time'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gioXIBuTR9fE","colab_type":"text"},"source":["# 4. Create DataFrame ```dynamic_features```\n","Here we create the data frame for dynamic features out of the orders dataframe. Following the orders dataframe can only be used to add something to this dataframe. \n","As indexes it has the date as well as the itemID."]},{"cell_type":"code","metadata":{"id":"ju8Ifv41KQy6","colab_type":"code","colab":{}},"source":["orders['date'] = orders['time'].dt.date\n","dynamic_feature = orders.groupby(['itemID','date']).aggregate({'order':'sum', 'salesPrice':'mean'})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3BbLsru2f9WC","colab_type":"code","colab":{}},"source":["# Create a new dataframe to store time-dependent data for items\n","_begin = min(orders['time']).date()\n","_end = max(orders['time']).date()\n","timespan = [p.date().strftime('%Y%m%d') for p in pd.date_range(start=_begin, end=_end)]\n","idx=  pd.MultiIndex.from_product([items.index, pd.to_datetime(timespan)], names=['itemID', 'date'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sDd_zfpWUOwS","colab_type":"code","colab":{}},"source":["dynamic_feature = pd.DataFrame(dynamic_feature, index=idx)\n","dynamic_feature['order'] = dynamic_feature['order'].fillna(0)\n","dynamic_feature['salesPrice'] = dynamic_feature['salesPrice'].fillna(method=\"bfill\")\n","dynamic_feature['salesPrice'] = dynamic_feature['salesPrice'].fillna(method=\"ffill\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C5r6-xa-Ma4w","colab_type":"text"},"source":["# 5. Deviation of Promotions and Creation of Test Set"]},{"cell_type":"markdown","metadata":{"id":"FbUMiSClZagQ","colab_type":"text"},"source":["Here we derive the promotions using the following method. This needs to be done to be able to have promotions in the test set when we manipulate it. "]},{"cell_type":"markdown","metadata":{"id":"rTBPYwXoFYob","colab_type":"text"},"source":["#### Promotion Derivation Functions\n","\n","The following functions can be used for deriving the promotions for test set. However, these function can also be used later to derive promotions for train set."]},{"cell_type":"code","metadata":{"id":"NpPByV2caf7_","colab_type":"code","colab":{}},"source":["def add_promotions_via_log2_new(lam=1, limit=2.5, **kwargs)->pd.DataFrame:\n","  \"\"\"\n","  data = timedep\n","  Predicts when items were promoted according to number of sold items on a daily\n","  basis.\n","  As promotions are marked all days where the follwing holds:\n","  log2((sold_on_current_day + lambda) / (sold_on_prev_day + lambda)) > limit\n","  \"\"\"\n","  def promotion_function(data: pd.DataFrame, ors: pd.DataFrame=None):\n","    print(\"... adding promotions\")\n","    sold_shifted = data['order'].shift(periods=1)\n","    trend = np.log2((data['order'] + lam) / (sold_shifted + lam))\n","    data['promoted'] = trend > limit\n","    print(f\"..... There are {sum(data['promoted'])} promotions derived\")\n","    return data\n","  return promotion_function"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uCMG6WezFXvD","colab_type":"code","colab":{}},"source":["# OLD FUNCTION\n","def add_promotions_via_log2(data: pd.DataFrame, ors: pd.DataFrame, lam=1, **kwargs)->dict:\n","  \"\"\"\n","  data = timedep\n","  Predicts when items were promoted according to number of sold items on a daily\n","  basis.\n","  As promotions are marked all days where the follwing holds:\n","  log2((sold_on_current_day + lambda) / (sold_on_prev_day + lambda)) > limit\n","  \"\"\"\n","  print(\"... adding promotions\")\n","  # Add the timestamp as index\n","  ors = ors.set_index(pd.DatetimeIndex(ors['time']))\n","\n","  # Hyperparameter for smoothing\n","  idx = None\n","\n","  grouped = ors.groupby(['itemID'])\n","  for item_id, group in grouped:\n","    if item_id % 500 == 0:\n","      print(f\"   ... {item_id}/{len(grouped)}\")\n","    sold_each_day = group['order'].resample('D').sum()\n","    first_day = sold_each_day.index[0]\n","\n","    if first_day.day == 1 and first_day.month == 1:\n","      # This is the first day of the simulation, the day before is unknown\n","      fill_value = np.nan\n","    else:\n","      # The item was not bought on the day before\n","      fill_value = 0\n","    \n","    # Use log difference (see https://www.youtube.com/watch?v=_N88aMUjDb8&feature=youtu.be, ~8m15s)\n","    # Shift for one day\n","    sold_each_day_shifted = sold_each_day.shift(periods=1, fill_value=fill_value)\n","\n","    trend = np.log2((sold_each_day + lam) / (sold_each_day_shifted + lam))\n","\n","    promoted = trend[trend > 2.5]\n","    promoted_dates = list(map(lambda x: x.to_pydatetime().strftime('%Y%m%d'), promoted.index))\n","\n","    _idx = pd.MultiIndex.from_product([[item_id], promoted_dates])\n","    if idx is None:\n","      idx = _idx\n","    else:\n","      idx = idx.union(_idx)\n","  \n","  data.loc[idx, 'promoted'] = True\n","  # data.fillna(value={'promoted': False}, inplace=True)\n","  data['promoted'].fillna(False, inplace=True)\n","\n","  return data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZPVQyTTXauen","colab_type":"code","colab":{}},"source":["def add_promotions_via_naive_mean_deviation(data, **kwargs):\n","  data[\"avg\"] = data[\"order\"].mean()\n","  data[\"promotion2\"] = 0\n","  data.loc[data[\"avg\"]< data[\"order\"],\"promotion2\"] = 1\n","  data.drop(columns=['avg'], inplace = True)\n","  return data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DsTLWvtDNhj1","colab_type":"text"},"source":["### Apply Promotion Derivation\n","\n","The following parameter defines which function is used to derive the promotions:"]},{"cell_type":"code","metadata":{"id":"lsgMSwwJNuoF","colab_type":"code","colab":{}},"source":["function_for_promotion: Callable = add_promotions_via_log2_new(limit=2.5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RcRr2l6cReP6","colab_type":"text"},"source":["---\n","Now we are adding the promotions to all of the data"]},{"cell_type":"code","metadata":{"id":"o5XgojdJNUqv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":50},"executionInfo":{"status":"ok","timestamp":1592835536773,"user_tz":-120,"elapsed":25799,"user":{"displayName":"Kyro Aiad","photoUrl":"","userId":"01120278881651803707"}},"outputId":"1e729181-ac0f-4a34-9631-3fc30618bb35"},"source":["dynamic_feature = function_for_promotion(data=dynamic_feature)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["... adding promotions\n","..... There are 17596 promotions derived\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KB2FkJYTe6cL","colab_type":"text"},"source":["#### Add Testing Columns\n","\n","Here we are also saving the orders which should be predicted. These are needed to check the performance of trained models."]},{"cell_type":"code","metadata":{"id":"fIa9klDue515","colab_type":"code","colab":{}},"source":["if make_final_submission:\n","  # Set test range\n","  test_range =  pd.date_range(start=final_submission_start, end=final_submission_end)\n","  # Create index for test\n","  idx = pd.MultiIndex.from_product([items.index, pd.to_datetime(test_range)], names=['itemID', 'date'])\n","  # Create DataFrame for the ending\n","  dynamic_feature_test = pd.DataFrame(index=idx, columns=dynamic_feature.columns)\n","\n","  # Add the simulation price\n","  simulation_price2 = pd.DataFrame(info['simulationPrice'].rename('salesPrice'))\n","  # Add simulation Price\n","  dynamic_feature_test = dynamic_feature_test.drop('salesPrice', axis=1).join(pd.DataFrame(simulation_price2), on='itemID')\n","  dynamic_feature_test['promoted'] = False\n","  \n","  # Add promotions\n","  for idx, row in info.iterrows():\n","    if not(str(row['promotion'])==\"nan\"):\n","      promotions = row['promotion'].split(\",\")\n","      for date in promotions:\n","        dynamic_feature_test.loc[(idx, pd.to_datetime(date)),'promoted']=True\n","    \n","else:\n","  #test_range = pd.date_range(start=test_period_start, end=test_period_end)\n","  dynamic_feature_test = dynamic_feature[dynamic_feature.index.get_level_values(1) >= pd.to_datetime(test_period_start)]\n","  dynamic_feature = dynamic_feature[dynamic_feature.index.get_level_values(1) < pd.to_datetime(test_period_start)]\n","  simulation_price = pd.DataFrame(dynamic_feature_test['salesPrice'].groupby('itemID').mean())\n","  dynamic_feature_test = dynamic_feature_test.drop('salesPrice', axis=1).join(pd.DataFrame(simulation_price), on='itemID')\n","  \n","  # Save the testing orders \n","  dynamic_feature_test.to_pickle(\"./data/orders_for_test.pkl\")\n","  \n","\n","  # Delete Order in test set\n","  dynamic_feature_test['order'] = np.NaN\n","\n","dynamic_feature = dynamic_feature.append(dynamic_feature_test)\n","\n","del dynamic_feature_test"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DCqvFZIu1x_q","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":431},"executionInfo":{"status":"ok","timestamp":1592835537289,"user_tz":-120,"elapsed":26270,"user":{"displayName":"Kyro Aiad","photoUrl":"","userId":"01120278881651803707"}},"outputId":"48b840a1-4aef-4930-cbab-1ff032cb5558"},"source":["dynamic_feature"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th></th>\n","      <th>order</th>\n","      <th>salesPrice</th>\n","      <th>promoted</th>\n","    </tr>\n","    <tr>\n","      <th>itemID</th>\n","      <th>date</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th rowspan=\"5\" valign=\"top\">1</th>\n","      <th>2018-01-01</th>\n","      <td>0.0</td>\n","      <td>3.11</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>2018-01-02</th>\n","      <td>0.0</td>\n","      <td>3.11</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>2018-01-03</th>\n","      <td>0.0</td>\n","      <td>3.11</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>2018-01-04</th>\n","      <td>0.0</td>\n","      <td>3.11</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>2018-01-05</th>\n","      <td>0.0</td>\n","      <td>3.11</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"5\" valign=\"top\">10463</th>\n","      <th>2018-06-25</th>\n","      <td>NaN</td>\n","      <td>282.16</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>2018-06-26</th>\n","      <td>NaN</td>\n","      <td>282.16</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>2018-06-27</th>\n","      <td>NaN</td>\n","      <td>282.16</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>2018-06-28</th>\n","      <td>NaN</td>\n","      <td>282.16</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>2018-06-29</th>\n","      <td>NaN</td>\n","      <td>282.16</td>\n","      <td>False</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1883340 rows × 3 columns</p>\n","</div>"],"text/plain":["                   order  salesPrice  promoted\n","itemID date                                   \n","1      2018-01-01    0.0        3.11     False\n","       2018-01-02    0.0        3.11     False\n","       2018-01-03    0.0        3.11     False\n","       2018-01-04    0.0        3.11     False\n","       2018-01-05    0.0        3.11     False\n","...                  ...         ...       ...\n","10463  2018-06-25    NaN      282.16     False\n","       2018-06-26    NaN      282.16     False\n","       2018-06-27    NaN      282.16     False\n","       2018-06-28    NaN      282.16     False\n","       2018-06-29    NaN      282.16     False\n","\n","[1883340 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"markdown","metadata":{"id":"U0t8-jUREx9b","colab_type":"text"},"source":["In the ```orders``` dataframe, we are not able to get information for prediction, because in later submission we don't neither. Therefore we need to delete rows, which define rows in the later notebook. "]},{"cell_type":"code","metadata":{"id":"LHy8U2zgE1Yl","colab_type":"code","colab":{}},"source":["if not make_final_submission:\n","    orders = orders[orders['time'] < test_period_start]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k1n41xs2In7a","colab_type":"text"},"source":["# 6. Defining Feature Functions\n","\n","In this area new features can be defined. A feature function takes the data as argument and returns the mutated dataframe. Following attributes can be used additionally:\n","\n","- ```ors```: The orders dataframe\n","- ```its```: The items dataframe\n","\n","__Why do we need functions?__\n","\n","This has three main reasons:\n","- We can easily add and delete functions\n","- We have an overview on which features are used currently\n","- The features we create here can be used in other projects as well \n","\n","__What if my function needs additional parameters?__\n","\n","Then create a function which returns a the final function. The following function returns a function which changes the sales price to the default value + 10.\n","\n","```\n","def test_function(default_value=1):\n","\n","  def final_function(data: pd.DataFrame):\n","    data['salesPrice'] = default_value + 10\n","\n","    return data\n","  return final_function\n","\n","```\n","Later this function can be triggered using following:\n","\n","```\n","test_function(default_value=10)(data)\n","```\n","This structure is important for later and enables adding the same feature with only different parameter.\n","\n","__How do I import features which are created by other regressors models or whatever?__\n","\n","Then please save the feature column in a seperate file. Then import the column in a function and then add it to the data like in the other feature function. In case that u simply import a file, specify how this feature is created, and please give a link or description on the location of the source code of the creation of this file. \n","Furthermore, please only import files, which are accessible by all of the team. So that in the end every team member is able to add the feature and run all of the pipeline. "]},{"cell_type":"markdown","metadata":{"id":"hP9bv_Vo8j08","colab_type":"text"},"source":["#### Has Rating Ordinal Feature\n","\n","Adds True if there is a rating."]},{"cell_type":"code","metadata":{"id":"DNp_GgxyyyN1","colab_type":"code","colab":{}},"source":["def add_has_rating_feature(data: dict, **kwargs)->dict:\n","  \"\"\"\n","  Ordinal feature, which returns if a method has a rating\n","  \"\"\"\n","  print(\"... applying has rating\")\n","  data['has_rating'] = data['customerRating']> 0\n","  return data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MVjly-4GIQTa","colab_type":"text"},"source":["#### Summed orders per item\n","\n","Feature which describes, how much was brought 2 weeks after this day. This feature can then be used for predicting the whole simulation period. "]},{"cell_type":"code","metadata":{"id":"iZVjq5VsKh4U","colab_type":"code","colab":{}},"source":["# dynamic_feature['order'].iloc[i]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iz-VaLayIPDk","colab_type":"code","colab":{}},"source":["#####\n","# UNDER CONSTRUCTION\n","# -- probably won't use \n","###\n","\n","# def summed_order_in_item(time_steps=14):\n","  \n","#   def feature_function(data,**kwargs):\n","#     test = data['order'].rolling(time_steps).sum()\n","#     return test\n","#   return feature_function\n","\n","\n","# this function is not applied at the very end, do we still want to use it?\n","# the number could be wrong as it's not grouped by each item \n","# also need to fill NaN values at the very beginning for the rolling time_steps "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zp9GRST-Iv5u","colab_type":"code","colab":{}},"source":["#summed_order_in_item()(data=dynamic_feature)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zRkLDb0C8jwG","colab_type":"text"},"source":["#### Add Divided Time Feature\n","\n","Adds time information:\n","- Timestamp\n","- Time\n","- Date\n","- Hour\n","- Day_of_year\n","- Day_of_month\n","- Month\n","- Week_nr\n","- Day_of_Week"]},{"cell_type":"code","metadata":{"id":"72z9_x0PHf2n","colab_type":"code","colab":{}},"source":["def add_divided_time_feature(data: dict, **kwargs)->dict:\n","  \"\"\"\n","  data = dynamic_feature\n","\n","  Divides the timestamp of the data into different features like the weekday etc.\n","  \"\"\"\n","  print(\"... adding time features\")\n","\n","  date = data.index.get_level_values(1)\n","\n","  data['day_of_year'] = date.dayofyear\n","  data['day_of_month'] = date.day\n","  data['month'] = date.month\n","  data['week_nr'] = date.week # updated here, week_nr should be int instead of str\n","  data['day_of_week'] = date.dayofweek # int(Monday:0 Sunday:6)\n","  return data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IBbiuY7h9E_S","colab_type":"text"},"source":["#### Aggregate Sold Feature\n","\n","Adds daily_sold, weekly_sold and monthly_sold items"]},{"cell_type":"code","metadata":{"id":"dLaCp4pBG83K","colab_type":"code","colab":{}},"source":["def add_aggregated_sold_feature(data:dict, **kwargs)->dict: \n","  print('... applying sold features')\n","  # add sold per day/week/month column to orders\n","  data = data.assign(daily_sold = data.groupby(['date', 'itemID']).order.transform('sum'))\n","  data = data.assign(weekly_sold = data.groupby(['week_nr', 'itemID']).order.transform('sum'))\n","  data = data.assign(monthly_sold = data.groupby(['month', 'itemID']).order.transform('sum'))\n","  return data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HQnny_029frK","colab_type":"text"},"source":["####Add_avg_salesPrice\n","Adds the average salesprice over the whole training and validation_period\n","\n","TODO: Check if not also test period is within -> If this is the case, its not tragic cause we use the sim_price for sales price in testing"]},{"cell_type":"code","metadata":{"id":"yZa1Cs79ug0R","colab_type":"code","colab":{}},"source":["def add_avg_salesPrice(data, ors, **kwargs):\n","  \"\"\"\n","  data = its\n","  Calculates the avg sales price over the whole training and validation period\n","  \"\"\"\n","  print(\"... adding avg sales price\")\n","\n","  # Calculate average over all orders (non-unique)\n","  avg_salesPrice_nonunique = ors[['salesPrice', 'itemID']].groupby('itemID').mean().rename(columns={'salesPrice':'avg_salesPrice_nonunique'})\n","  data = data.join(avg_salesPrice_nonunique, on=\"itemID\")\n","  data['avg_salesPrice_nonunique'].fillna(data['recommendedRetailPrice'], inplace=True)\n","  return data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iCwUGvdo93X5","colab_type":"text"},"source":["#### Add Avg Daily Sales Price\n","\n","Adds the daily sales price for each item"]},{"cell_type":"code","metadata":{"id":"5gIFf--OcI80","colab_type":"code","colab":{}},"source":["def add_avg_daily_salesPrice(data, ors, its, **kwargs):\n","  \"\"\"\n","  data = timedep\n","  Calculates the average of (unique) prices for every day.\n","  \"\"\"\n","  print(\"... adding daily sales price\")\n","\n","  grouped = ors.groupby(['itemID', 'date'])['salesPrice']\n","\n","  # Add average of unique prices\n","  daily_price_item = grouped.unique().apply(lambda x: np.mean(x))\n","  data.loc[daily_price_item.index, 'avg_daily_salesPrice'] = daily_price_item\n","\n","  # Add average of all prices (non-unique)\n","  daily_price_item_nonunique = grouped.mean()\n","  data.loc[daily_price_item_nonunique.index, 'avg_daily_salesPrice_nonunique'] = daily_price_item_nonunique\n","\n","  # If value is missing, first check if there is average of the whole period. If not, use recommended retail price\n","  data = data.groupby('itemID').apply(lambda group: group.fillna(its.loc[group.name].avg_salesPrice_nonunique))\n","  data = data.groupby('itemID').apply(lambda group: group.fillna(its.loc[group.name].recommendedRetailPrice))\n","\n","  return data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5C-Kk0By9_WG","colab_type":"text"},"source":["#### Add Avg of daily salesPrice\n","Calculate average over daily averages."]},{"cell_type":"code","metadata":{"id":"oCXR7-QEVp2V","colab_type":"code","colab":{}},"source":["def add_avg_of_daily_salesPrice(data, td, **kwargs):\n","  \"\"\"\n","  data = items\n","  Calculate average over daily averages (nonunique)\n","  \"\"\"\n","  avg = td.groupby('itemID')['avg_daily_salesPrice_nonunique'].mean()\n","  data.loc[avg.index, 'avg_salesPrice'] = avg\n","  return data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-5Bvfxlv-JqH","colab_type":"text"},"source":["#### Add Avg Daily Sold\n","  data = items\n","  \n","  Calculate avg daily sold and standard deviation.\n"]},{"cell_type":"code","metadata":{"id":"2uTjSV3s5jgv","colab_type":"code","colab":{}},"source":["def add_avg_daily_sold(data, ors, **kwargs):\n","  \"\"\"\n","  data = items\n","  Calculate avg daily sold and standard deviation.\n","\n","  \"\"\"\n","  print(\"... adding avg daily sold\")\n","  nr_days_training = (ors['time'].max() - ors['time'].min()).days + 1\n","  daily_sold = ors.groupby(['itemID', 'date']).order.sum()\n","  grouped = daily_sold.groupby('itemID')\n","\n","  # Avg over all training period\n","  avg_sold_item = (grouped.sum() / nr_days_training)\n","  data.loc[avg_sold_item.index, 'avg_sold_day'] = avg_sold_item\n","\n","  # Avg over days where total number of orders > 0\n","  avg_sold_item_only_nonzero = grouped.mean()\n","  data.loc[avg_sold_item_only_nonzero.index, 'avg_sold_day_only_nonzero'] = avg_sold_item_only_nonzero\n","  \n","  # Std over days where total number of orders > 0\n","  std_sold_item_only_nonzero = grouped.std()\n","  data.loc[std_sold_item_only_nonzero.index, 'std_sold_day_only_nonzero'] = std_sold_item_only_nonzero\n","\n","  # Fill missing values\n","  data['avg_sold_day'].fillna(0, inplace=True)\n","  data['avg_sold_day_only_nonzero'].fillna(0, inplace=True)\n","  data['std_sold_day_only_nonzero'].fillna(0, inplace=True)\n","  return data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tCo7uR9z-Xl6","colab_type":"text"},"source":["#### Add_SalesPrice_Over_Price\n","TODO: Please add description and assertions for wrong order (Just test if add_avg_daily_salesPrice_nonunique & add_avg_salesPrice are present, if not tell that the other functions need to be executed before)\n","\n","TODO: The function which should be executed, are not existing!\n","\n","@ Markus, the function for creating avg_daily_salesPrice_nonunique indeed does exist (check out function add_avg_daily_salesPrice()), also avg_salesPrice should also be created within the function add_avg_of_daily_salesPrice(). avg_salesPrice should be the average of avg_daily_salesPrice_nonunique. I hope it makes sense (Min). "]},{"cell_type":"code","metadata":{"id":"FsmwtN0-F1gI","colab_type":"code","colab":{}},"source":["# Execute after: def add_avg_of_daily_salesPrice & add_avg_of_daily_salesPrice\n","\n","# I added \"salesPrice_over_avg_salesPrice\" and \"salesPrice_over_avg_salesPrice_nonunique\" (Min)\n","\n","def add_salesPrice_over_price(data, its, **kwargs):\n","  \"\"\"\n","  data = timedep or dynamic features\n","  calculate relationships between price of that day and [recommendedRetailPrice, avg_salesPrice, and avg_salesPrice_nonunique]\n","  \"\"\"\n","  print(\"... applying add_salesPrice_over_price\")\n","\n","  df_sales = data.join(its, on = 'itemID', how = 'left')\n","  data['salesPrice_over_recommended'] = df_sales['avg_daily_salesPrice_nonunique'] / df_sales['recommendedRetailPrice']\n","  data['salesPrice_over_avg_salesPrice'] = df_sales['avg_daily_salesPrice_nonunique'] / df_sales['avg_salesPrice']\n","  data['salesPrice_over_avg_salesPrice_nonunique'] = df_sales['avg_daily_salesPrice_nonunique'] / df_sales['avg_salesPrice_nonunique']\n","  \n","  data['salesPrice_over_recommended'].fillna(0, inplace=True)\n","  data['salesPrice_over_avg_salesPrice'].fillna(0, inplace=True)\n","  data['salesPrice_over_avg_salesPrice_nonunique'].fillna(0, inplace = True)\n","\n","  return data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_VSjU103n9-E","colab_type":"text"},"source":["#### Add German holidays\n","\n","Method to add German Holidays"]},{"cell_type":"code","metadata":{"id":"MEx-e5f8eHNP","colab_type":"code","colab":{}},"source":["def add_german_holidays(data, **kwargs):\n","  \n","  import holidays\n","  print(\"... applying add_german_holidays\")\n","  de_holidays = holidays.DE(years= 2018) #Holidays Library\n","  a = pd.DataFrame(de_holidays.keys())\n","\n","  # Creating bool: if date is holiday\n","  data['de_holidays'] = data.index.get_level_values(1).isin(a)\n","\n","  return data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BNONyB0_-tcd","colab_type":"text"},"source":["#### Add Has Promotion on Day\n","\n","Adds a column to orders that tells whether the item in this row has been \n","  promoted on that day."]},{"cell_type":"code","metadata":{"id":"mPfUQFIfJEvm","colab_type":"code","colab":{}},"source":["def add_has_promotion_on_day(data: dict, its: dict, **kwargs):\n","  \"\"\"\n","  Adds a column to orders that tells whether the item in this row has been \n","  promoted on that day.\n","  \"\"\"\n","  print(\"... applying add_has_promotion_on_day\")\n","  data = data.assign(has_promotions_this_day=False)\n","  grouped = data.groupby(['itemID', 'date'])\n","  for (item_id, date), group in grouped:\n","    date_correct_format = group.iloc[0]['time'].to_pydatetime().strftime('%Y-%m-%d')\n","    if date_correct_format in its.loc[item_id].promotion_derived.split(','):\n","      data.loc[group.index, ['has_promotions_this_day']] = True\n","  return data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RhKrR63jQ6Vr","colab_type":"text"},"source":["## Num Promotions"]},{"cell_type":"code","metadata":{"id":"hkKIdrV4Q9XQ","colab_type":"code","colab":{}},"source":["def num_promotions(data, td, its, **kwargs):\n","  print(\"... adding num promotions\")\n","  num_promotions = td[td['promoted']==True].groupby('itemID')['promoted'].count()\n","  num_promotions.name = 'num_promotions'\n","  data = data.join(num_promotions, rsuffix=\"_num_promotions\")\n","  data['num_promotions'].fillna(value=0,inplace=True)\n","  return data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zE_W9OwjRI1R","colab_type":"text"},"source":["## Promotion in Data"]},{"cell_type":"code","metadata":{"id":"x6G6lxYkRLrw","colab_type":"code","colab":{}},"source":["# add after num_promotions\n","def promotion_in_data(data, td, its, **kwargs):\n","  print(\"... adding promotion in data\")\n","  print(td)\n","  data['promotion_in_data'] = data['num_promotions'].astype('bool')\n","  return data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TC_-YTlaRQNh","colab_type":"text"},"source":["## Avg Sold Promoted"]},{"cell_type":"code","metadata":{"id":"gnUK33K6RTap","colab_type":"code","colab":{}},"source":["def avg_sold_promoted(data, td, its, **kwargs):\n","  print(\"... adding avg_sold_promoted\")\n","  promoted = td[td['promoted']==True].groupby('itemID')['order'].mean()\n","  promoted.name = 'avg_sold_promoted'\n","  data = data.join(promoted, rsuffix=\"_promoted\")\n","  data['avg_sold_promoted'].fillna(value=data['avg_sold_promoted'].mean(),inplace=True)\n","  return data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mzSoVll1RZBP","colab_type":"text"},"source":["## Avg Sold Not Promoted"]},{"cell_type":"code","metadata":{"id":"ji__5b0gRWZq","colab_type":"code","colab":{}},"source":["def avg_sold_not_promoted(data, td, its, **kwargs):\n","  print(\"... adding avg_sold_not_promoted\")\n","  not_promoted = td[td['promoted']==False].groupby('itemID')['order'].mean()\n","  not_promoted.name = 'avg_sold_not_promoted'\n","  data = data.join(not_promoted)\n","  data['avg_sold_not_promoted'].fillna(value=data['avg_sold_not_promoted'].mean(),inplace=True)\n","  return data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QdSSzCJ6RfMc","colab_type":"text"},"source":["## Last promotions"]},{"cell_type":"code","metadata":{"id":"VVG3NdAlRjjP","colab_type":"code","colab":{}},"source":["def last_promotion(item, avg_sales_of_all_items):\n","  itemID = item.index.values[0][0]\n","  if itemID % 1000 == 0:\n","    print(itemID)\n","\n","  first_day_of_data = item.index.get_level_values(1).to_list()[0]\n","  last_day_of_data = item.index.get_level_values(1).to_list()[-1]\n","\n","  def get_avg_order_on_promo_day(promos):\n","    idx = pd.MultiIndex.from_product([[itemID], promos])\n","    orders_all = item.loc[idx]\n","    orders = orders_all[orders_all.order.notna()].order\n","    if len(orders) > 0:\n","      avg = orders.mean()\n","    else:\n","      avg = avg_sales_of_all_items\n","    return round(avg)\n","\n","  def helper(start_date, end_date, default_sales=None):\n","    # Set values to entries between start_date and end_date\n","    days_between = (end_date - start_date).days\n","    first_day_to_set = start_date + timedelta(days=1)  # First day after last promotion\n","\n","    if first_day_to_set < first_day_of_data:\n","      value_for_january_1st = (first_day_of_data - first_day_to_set).days + 1\n","      first_day_to_set = first_day_of_data\n","      days_since_last_promo = np.arange(value_for_january_1st, days_between+1)\n","      last_promo_sales = default_sales  # Avg of promotions for this item\n","    else:\n","      days_since_last_promo = np.arange(1, days_between+1)  # A list of increasing numbers, starting with 1\n","      last_promo_sales = item.loc[(itemID, start_date)].order  # Sales on the last promoted day\n","\n","    item.loc[(itemID, first_day_to_set):(itemID, end_date), \"days_since_last_promotion\"] = days_since_last_promo    \n","    item.loc[(itemID, first_day_to_set):(itemID, end_date), \"last_promoted_sales\"] = last_promo_sales\n","    return item\n","\n","  # Get list of dates when the item was promoted\n","  promotions = item.index[item.promoted].get_level_values(1).to_list()\n","\n","  default_sales = get_avg_order_on_promo_day(promotions)\n","\n","  last_promo = datetime(year=2017, month=12, day=1)  # First imaginary promotion\n","\n","  if len(promotions) == 0:\n","    item = helper(last_promo, last_day_of_data, default_sales)\n","    return item\n","\n","  for promo_day in promotions:\n","    # Loop through all promotions and set all intermediate values at once\n","    item = helper(last_promo, promo_day, default_sales)\n","    last_promo = promo_day\n","\n","  # Add values for the days after the last promotion\n","  \n","  if (last_day_of_data - last_promo).days > 0:\n","    helper(last_promo, last_day_of_data)\n","\n","  return item"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5fCXQjq5Vw-l","colab_type":"code","colab":{}},"source":["def last_promotions(data, **kwargs):\n","  print(\"... adding last_promotions\")\n","  data[\"days_since_last_promotion\"] = np.nan\n","  data[\"last_promoted_sales\"] = np.nan\n","  avg_sales_of_all_items = data[data['promoted']==True].groupby('itemID')['order'].mean().mean()\n","  data = data.groupby('itemID').apply(last_promotion, avg_sales_of_all_items = avg_sales_of_all_items)\n","  return data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Yx_99yWPk7b2","colab_type":"text"},"source":["## Creating Price Clusters\n","\n","Creation of price buckets"]},{"cell_type":"code","metadata":{"id":"X66ezrF2k5_a","colab_type":"code","colab":{}},"source":["# for recommendedRetailprice in items\n","def add_price_bins(n_bins: int, field: str):\n","  def bin_fct(data, **kwargs):\n","    print(f\"... applying price bins on {field} with {n_bins} bins\")\n","    data[f'{field}_bucket'] = pd.qcut(data[field], n_bins, labels=range(0, n_bins))\n","    return data\n","  return bin_fct"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V0ZW8cI3mJe4","colab_type":"text"},"source":["## Creating Price/Order Clusters"]},{"cell_type":"code","metadata":{"id":"0E3Wz5NdmcaL","colab_type":"code","colab":{}},"source":["# Using sum of all orders per item\n","def add_price_sumOrder_cluster(data, ors, **kwargs):\n","  # implement number of orders from order file \n","  sold_each_item = ors[['itemID', 'order']].groupby(['itemID']).sum()\n","  data = data.reset_index()\n","  data = data.join(sold_each_item, on=\"itemID\")\n","  # Clustering\n","  n_bins = 3\n","  data['priceCluster'] = pd.qcut(data.recommendedRetailPrice, n_bins, labels=range(0, n_bins))\n","  data['orderCluster'] = pd.qcut(data.order, n_bins, labels=range(0, n_bins))\n","  # CAUSES ALL THE TROUBLE: drop all rows with NaN in orderCluster  \n","  #data = data[data['orderCluster'].notna()]\n","  \n","  # Combining price and order Cluster\n","  def combi_pr_or(row):\n","      global val\n","      if row['priceCluster'] == 0 and row['orderCluster'] == 0:\n","          val = 1 #cheap and sold rarely\n","      elif row['priceCluster'] == 0 and row['orderCluster'] == 1:\n","          val = 2 #cheap and sold middle\n","      elif row['priceCluster'] == 0 and row['orderCluster'] == 2:\n","          val = 3 #cheap and sold often\n","      elif row['priceCluster'] == 1 and row['orderCluster'] == 0:\n","          val = 4 #middle price and sold rarely\n","      elif row['priceCluster'] == 1 and row['orderCluster'] == 1:\n","          val = 5 #middle price and sold middle\n","      elif row['priceCluster'] == 1 and row['orderCluster'] == 2:\n","          val = 6 #middle price and sold often\n","      elif row['priceCluster'] == 2 and row['orderCluster'] == 0:\n","          val = 7 #expensive and sold rarely\n","      elif row['priceCluster'] == 2 and row['orderCluster'] == 1:\n","          val = 8 #expensive and sold middle\n","      elif row['priceCluster'] == 2 and row['orderCluster'] == 2:\n","          val = 9 #expensive and sold often\n","      return val\n","\n","  # Combine both Cluster with combi_pr_or function\n","  data['price_sumSales_cluster'] = data.apply(combi_pr_or, axis=1)\n","  data = data.drop(['priceCluster', 'orderCluster', 'order'], axis=1)\n","  data = data.set_index(\"itemID\")\n","\n","  return data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nMxy_8dh2U8K","colab_type":"code","colab":{}},"source":["# Using max of Order per item\n","def add_price_maxOrder_cluster(data, ors, **kwargs):\n","  # find max order for every itemID in save as order in max_orders\n","  max_orders = ors.loc[ors.reset_index().groupby(['itemID'])['order'].idxmax()]\n","  data = data.reset_index()\n","  data = pd.merge(data, max_orders[['order', 'itemID']], on=\"itemID\", how='left')\n","  # Clustering\n","  n_bins = 3\n","  data['priceCluster'] = pd.qcut(data.recommendedRetailPrice, n_bins, labels=range(0, n_bins))\n","  data['orderCluster'] = pd.qcut(data.order, n_bins+1, labels=range(0, n_bins), duplicates='drop')\n","  \n","  # drop all rows with NaN in orderCluster  \n","  data = data[data['orderCluster'].notna()]\n","\n","  # Combining price and order Cluster\n","  def combi_pr_or(row):\n","      global val\n","      if row['priceCluster'] == 0 and row['orderCluster'] == 0:\n","          val = 1 #cheap and sold rarely\n","      elif row['priceCluster'] == 0 and row['orderCluster'] == 1:\n","          val = 2 #cheap and sold middle\n","      elif row['priceCluster'] == 0 and row['orderCluster'] == 2:\n","          val = 3 #cheap and sold often\n","      elif row['priceCluster'] == 1 and row['orderCluster'] == 0:\n","          val = 4 #middle price and sold rarely\n","      elif row['priceCluster'] == 1 and row['orderCluster'] == 1:\n","          val = 5 #middle price and sold middle\n","      elif row['priceCluster'] == 1 and row['orderCluster'] == 2:\n","          val = 6 #middle price and sold often\n","      elif row['priceCluster'] == 2 and row['orderCluster'] == 0:\n","          val = 7 #expensive and sold rarely\n","      elif row['priceCluster'] == 2 and row['orderCluster'] == 1:\n","          val = 8 #expensive and sold middle\n","      elif row['priceCluster'] == 2 and row['orderCluster'] == 2:\n","          val = 9 #expensive and sold often\n","      elif row['priceCluster'] == 0 and row['orderCluster'] == 3:\n","          val = 10 #expensive and sold often\n","      elif row['priceCluster'] == 1 and row['orderCluster'] == 3:\n","          val = 11 #expensive and sold often\n","      elif row['priceCluster'] == 2 and row['orderCluster'] == 3:\n","          val = 12 #expensive and sold often'''\n","      return val\n","    \n","  # Combine both Cluster with combi_pr_or function\n","  data['price_maxSales_cluster'] = data.apply(combi_pr_or, axis=1)\n","  data = data.drop(['priceCluster', 'order'], axis=1)\n","  data = data.set_index(\"itemID\")\n","\n","  return data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uI2a40EUbPCL","colab_type":"text"},"source":["#### Clusters for items that were frequently bought together"]},{"cell_type":"code","metadata":{"id":"cIhOagXXbOe1","colab_type":"code","colab":{}},"source":["def add_bought_together_clusters(data, **kwargs):\n","  \"\"\"\n","  data = items\n","\n","  Items that were \"often\" bought together are in the same cluster.\n","  What \"often\" means can be changed in the nb linked below. The clusters that \n","  are currently used are based on parameters (10, 40): \n","  - two items must be bought together at least 40 times.\n","  - at least 10 percentiles of times when one item was bought, the other item \n","    was also bought.\n","  (the described approach is similar to apriori algorithm)\n","\n","  I made the relation transitive: if A together with B and B together with C,\n","  then A together with C, which means that all three (A, B, C) are in the same \n","  cluster.\n","\n","  Cluster value -1 means no cluster (the item was rarely or never bought \n","  together with something else).\n","\n","  Link to the nb where this is derived: https://colab.research.google.com/drive/1L0LEBlxD-X9wvyH28XQNvQTmQDq68sYl\n","  \"\"\"\n","  print(\"... adding bought_together_cluster\")\n","  clusters = pd.read_pickle('./data/bought_together.pk')\n","  data['bought_together_cluster'] = clusters['together_10_40']\n","  return data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tiIEb-VYCvIG","colab_type":"code","colab":{}},"source":["def add_two_days_after_promo(data, **kwargs):\n","  \"\"\"\n","  add boolean variable for if date is one or two days after promo, as after an auction items stay promoted for 44 hours\n","  \"\"\"\n","  data.reset_index(inplace=True)\n","  data.set_index('itemID', inplace = True)\n","  data[\"promotion_lag1\"] = data.groupby(data.index).promoted.shift(1)\n","  data[\"promotion_lag2\"] = data.groupby(data.index).promoted.shift(2)\n","  data[\"promotion_lag1\"].fillna(False, inplace = True)\n","  data[\"promotion_lag2\"].fillna(False, inplace = True)\n","  data[\"two_days_after_promo\"] = data[\"promotion_lag1\"]\n","  data.loc[data[\"promotion_lag2\"]==True,\"two_days_after_promo\"] = True\n","  data.reset_index(inplace=True)\n","  data.set_index([\"itemID\",\"date\"],inplace=True)\n","\n","  return data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YgFvWAofYCQw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1592835537729,"user_tz":-120,"elapsed":26262,"user":{"displayName":"Kyro Aiad","photoUrl":"","userId":"01120278881651803707"}},"outputId":"52325da7-4eb8-4f17-9077-dc3a4415b232"},"source":["'''def add_total_sales_and_promos(data, **kwargs):\n","  \"\"\"\n","  Add function for total sales and total promotions per day\n","  \"\"\"\n","  data2 = data.reset_index(drop=True)\n","  data2 = data.groupby([\"date\"]).agg({'order': 'sum','promoted': 'sum'})\n","  data2.rename(columns = {'order':'total_sales','promoted':'count_promotions'})\n","  data = data.merge(data2, on='date',how='left')\n","  \n","  return data'''"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'def add_total_sales_and_promos(data, **kwargs):\\n  \"\"\"\\n  Add function for total sales and total promotions per day\\n  \"\"\"\\n  data2 = data.reset_index(drop=True)\\n  data2 = data.groupby([\"date\"]).agg({\\'order\\': \\'sum\\',\\'promoted\\': \\'sum\\'})\\n  data2.rename(columns = {\\'order\\':\\'total_sales\\',\\'promoted\\':\\'count_promotions\\'})\\n  data = data.merge(data2, on=\\'date\\',how=\\'left\\')\\n  \\n  return data'"]},"metadata":{"tags":[]},"execution_count":44}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"txZPXp1IvzN0","colab":{}},"source":["def add_total_sales_and_promos(data, **kwargs):\n","  \"\"\"\n","  Add function for total sales and total promotions per day\n","  \"\"\"\n","  data2 = data.reset_index()\n","  data2 = data2[['date', 'order', 'promoted']]\n","  data2 = data2.groupby([\"date\"]).sum()\n","  data2 = data2.rename(columns = {'order':'total_sales','promoted':'count_promotions'})\n","  \n","  data = data.reset_index()\n","  data = data.join(data2, on='date',how='left')\n","  data = data.set_index([\"itemID\",\"date\"])\n","  return data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"22oRO65AMseU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":431},"executionInfo":{"status":"ok","timestamp":1592835538109,"user_tz":-120,"elapsed":26604,"user":{"displayName":"Kyro Aiad","photoUrl":"","userId":"01120278881651803707"}},"outputId":"62d141ae-1111-4863-d08b-b3c7c35088e7"},"source":["add_total_sales_and_promos(dynamic_feature)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th></th>\n","      <th>order</th>\n","      <th>salesPrice</th>\n","      <th>promoted</th>\n","      <th>total_sales</th>\n","      <th>count_promotions</th>\n","    </tr>\n","    <tr>\n","      <th>itemID</th>\n","      <th>date</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th rowspan=\"5\" valign=\"top\">1</th>\n","      <th>2018-01-01</th>\n","      <td>0.0</td>\n","      <td>3.11</td>\n","      <td>False</td>\n","      <td>6305.0</td>\n","      <td>36.0</td>\n","    </tr>\n","    <tr>\n","      <th>2018-01-02</th>\n","      <td>0.0</td>\n","      <td>3.11</td>\n","      <td>False</td>\n","      <td>6165.0</td>\n","      <td>46.0</td>\n","    </tr>\n","    <tr>\n","      <th>2018-01-03</th>\n","      <td>0.0</td>\n","      <td>3.11</td>\n","      <td>False</td>\n","      <td>2821.0</td>\n","      <td>24.0</td>\n","    </tr>\n","    <tr>\n","      <th>2018-01-04</th>\n","      <td>0.0</td>\n","      <td>3.11</td>\n","      <td>False</td>\n","      <td>10819.0</td>\n","      <td>54.0</td>\n","    </tr>\n","    <tr>\n","      <th>2018-01-05</th>\n","      <td>0.0</td>\n","      <td>3.11</td>\n","      <td>False</td>\n","      <td>11465.0</td>\n","      <td>41.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"5\" valign=\"top\">10463</th>\n","      <th>2018-06-25</th>\n","      <td>NaN</td>\n","      <td>282.16</td>\n","      <td>False</td>\n","      <td>0.0</td>\n","      <td>110.0</td>\n","    </tr>\n","    <tr>\n","      <th>2018-06-26</th>\n","      <td>NaN</td>\n","      <td>282.16</td>\n","      <td>False</td>\n","      <td>0.0</td>\n","      <td>172.0</td>\n","    </tr>\n","    <tr>\n","      <th>2018-06-27</th>\n","      <td>NaN</td>\n","      <td>282.16</td>\n","      <td>False</td>\n","      <td>0.0</td>\n","      <td>140.0</td>\n","    </tr>\n","    <tr>\n","      <th>2018-06-28</th>\n","      <td>NaN</td>\n","      <td>282.16</td>\n","      <td>False</td>\n","      <td>0.0</td>\n","      <td>119.0</td>\n","    </tr>\n","    <tr>\n","      <th>2018-06-29</th>\n","      <td>NaN</td>\n","      <td>282.16</td>\n","      <td>False</td>\n","      <td>0.0</td>\n","      <td>162.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1883340 rows × 5 columns</p>\n","</div>"],"text/plain":["                   order  salesPrice  promoted  total_sales  count_promotions\n","itemID date                                                                  \n","1      2018-01-01    0.0        3.11     False       6305.0              36.0\n","       2018-01-02    0.0        3.11     False       6165.0              46.0\n","       2018-01-03    0.0        3.11     False       2821.0              24.0\n","       2018-01-04    0.0        3.11     False      10819.0              54.0\n","       2018-01-05    0.0        3.11     False      11465.0              41.0\n","...                  ...         ...       ...          ...               ...\n","10463  2018-06-25    NaN      282.16     False          0.0             110.0\n","       2018-06-26    NaN      282.16     False          0.0             172.0\n","       2018-06-27    NaN      282.16     False          0.0             140.0\n","       2018-06-28    NaN      282.16     False          0.0             119.0\n","       2018-06-29    NaN      282.16     False          0.0             162.0\n","\n","[1883340 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":46}]},{"cell_type":"code","metadata":{"id":"JgWPK5sBIl9y","colab_type":"code","colab":{}},"source":["def forecast_total_sales(data, **kwargs):\n","  \"\"\"\n","  forecast total sales for simulation period using prophet\n","  \"\"\"\n","  #Prepare format for prophet\n","  data2 = data.reset_index(drop=True)\n","  data2 = data.groupby([\"date\"]).agg({'order': 'sum','promoted': 'sum'})\n","  data2.reset_index(inplace=True)\n","  data2.rename(columns = {'date':'ds','order':'y','promoted':'count_promotions'}, inplace = True) \n","\n","  #Prepare train and test data\n","  train = data2[:-14]\n","  test = data2[-14:]\n","\n","  #Make Prediction\n","  from fbprophet import Prophet\n","  prophet_basic = Prophet()\n","  prophet_basic.add_regressor('count_promotions')\n","  prophet_basic.fit(train)\n","  forecast = prophet_basic.predict(test)\n","\n","  #Add Forecast to original data\n","  data = data.reset_index()\n","  forecast['y'] = forecast['yhat']\n","  final_data = train.append(forecast[['ds','y','count_promotions']])\n","  final_data.rename(columns= {'ds':'date','y':'total_sales'}, inplace = True)\n","  data = data.merge(final_data, on='date',how='left')\n","  data.set_index(['itemID','date'], inplace = True)\n","  #data.drop(columns = ['level_0','index'], inplace = True)\n","  return data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SZtcgnQSbN--","colab_type":"text"},"source":["# 7. Apply Features\n","\n","Now here we can add the function which should be applied.\n","The function in the list, is executed in exactly this order. In case of a keyError it might be that one function is dependend on another function which is not yet executed. "]},{"cell_type":"markdown","metadata":{"id":"Ki-yZmbg_Ojl","colab_type":"text"},"source":["## Apply Function on Dynamic Features\n","\n","Here the function which should be applied on the 'dynamic features' dataframe are specified."]},{"cell_type":"code","metadata":{"id":"Ry97aLZv_TqH","colab_type":"code","colab":{}},"source":["applied_features_for_dynamic_features = [\n","                    #add_divided_time_feature,\n","                    #add_two_days_after_promo, \n","                    #add_total_sales_and_promos,\n","                    ##forecast_total_sales, #only for final submission\n","                    ##add_avg_daily_salesPrice, # add_avg_daily_salesPrice_nonunique mandatory, but not implemented -> this function \"add_avg_daily_salesPrice\" implements avg_daily_salesPrice_nonunique...\n","                    ###add_sold_per_period, # this fct doesn't exist? # Here order will be filled with nan -> Not good we need orders for test set to be Nan. -> more comments see function :: This fct doesn't exists?\n","                    #add_german_holidays,\n","                    ##add_salesPrice_over_price,# add_avg_daily_salesPrice_nonunique mandatory, but not implemented -> same comment as above in \"add_avg_daily_salesPrice\"\n","                    #last_promotions,\n","                    #add_price_bins(10, field='salesPrice')\n","]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r3mRADk-AzAW","colab_type":"text"},"source":["## Apply Function on Items Table\n","\n"]},{"cell_type":"code","metadata":{"id":"-2uFNjLKA5n0","colab_type":"code","colab":{}},"source":["applied_features_for_items = [\n","                    ## add_avg_salesPrice, # add directly before applying dynamic features, see next code cell\n","                    #add_has_rating_feature,\n","                    #add_price_bins(10,field='recommendedRetailPrice'),\n","                    ##add_avg_daily_sold,\n","                    #add_bought_together_clusters,\n","                    ## add_avg_of_daily_salesPrice, # add directly before applying dynamic features, see next code cell\n","                    #add_price_sumOrder_cluster,\n","                    add_price_maxOrder_cluster,\n","                    #num_promotions,  #columns overlap but no suffix specified: Index(['num_promotions'] \n","                    #promotion_in_data,\n","                    #avg_sold_promoted, # columns overlap but no suffix specified: Index(['avg_sold_promoted'], dtype='object') \n","                    #avg_sold_not_promoted\n","]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4o21GuNuT_xD","colab_type":"text"},"source":["## Technical Applying the features\n","This is the code to really use the above features."]},{"cell_type":"code","metadata":{"id":"n7_Qjw08DVLx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":67},"executionInfo":{"status":"ok","timestamp":1592835600369,"user_tz":-120,"elapsed":55873,"user":{"displayName":"Kyro Aiad","photoUrl":"","userId":"01120278881651803707"}},"outputId":"2e596def-4d2f-4621-d8ec-1df849b2f658"},"source":["# static features, need to run first since dynamic features depend on this\n","# dependent features, need to run in this order.. \n","\n","items = add_avg_salesPrice(items, orders)\n","dynamic_feature = add_avg_daily_salesPrice(dynamic_feature, orders, items)\n","items = add_avg_of_daily_salesPrice(items, dynamic_feature)\n","dynamic_feature = add_salesPrice_over_price(dynamic_feature, items)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["... adding avg sales price\n","... adding daily sales price\n","... applying add_salesPrice_over_price\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Lw5UqcGwD4lE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":262},"executionInfo":{"status":"error","timestamp":1592837121511,"user_tz":-120,"elapsed":3812,"user":{"displayName":"Kyro Aiad","photoUrl":"","userId":"01120278881651803707"}},"outputId":"a67a3246-5338-47ab-b490-3038c7886408"},"source":["def apply_features_training(data:dict, features: List[Callable])->dict:\n","  assert isinstance(features, list)\n","\n","  backup_data = data.copy()\n","  \n","  try:\n","    for feature in features:\n","      result = feature(data, ors=orders, its=items, td=dynamic_feature)\n","      assert result is not None, \"The function need to return the mutated data\"\n","      data = result\n","    return data\n","  except Exception as e:\n","    print(f\"\\nERROR:\\nIn Function {feature} was following Error:\\n\\n {e} \\n Now reverting the data\")\n","    return backup_data\n","\n","print(\"+++ ADDING DYNAMIC FEATURES+++ \\n _____________________________________________\")\n","dynamic_feature = apply_features_training(dynamic_feature, applied_features_for_dynamic_features)\n","print(\"+++ ADDING STATIC FEATURES+++ \\n _____________________________________________\")\n","items = apply_features_training(items, applied_features_for_items)\n","\n","assert dynamic_feature['order'].isnull().values.any()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+++ ADDING DYNAMIC FEATURES+++ \n"," _____________________________________________\n","+++ ADDING STATIC FEATURES+++ \n"," _____________________________________________\n"],"name":"stdout"},{"output_type":"error","ename":"AssertionError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-81-5a0c5e523708>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_features_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapplied_features_for_items\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mdynamic_feature\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'order'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mAssertionError\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"Nox428BDW7wm","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":185},"executionInfo":{"status":"ok","timestamp":1592837160221,"user_tz":-120,"elapsed":918,"user":{"displayName":"Kyro Aiad","photoUrl":"","userId":"01120278881651803707"}},"outputId":"663d6c92-ed3d-419a-87f6-bc7b1a165863"},"source":["items.price_maxSales_cluster.value_counts()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["7    2433\n","4    1468\n","3    1259\n","2     796\n","1     744\n","5     735\n","6     588\n","8     219\n","9     143\n","Name: price_maxSales_cluster, dtype: int64"]},"metadata":{"tags":[]},"execution_count":84}]},{"cell_type":"code","metadata":{"id":"GKEoonGOGYxe","colab_type":"code","colab":{}},"source":["# Here I will still apply the automated features (using featuretools) \n","# once all new features are created correctly in the static/items table (Min)\n","# this will build upon the features we created in items as well as the original orders table \n","# the newly created features will be added to \"static_features_final.pk\" "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2zk6CLGAN5Oe","colab_type":"text"},"source":["# Save dataset\n","Here the both datasets are saved.\n","\n","The following variable is used to save the data"]},{"cell_type":"code","metadata":{"id":"WQg1-SCEH4lU","colab_type":"code","colab":{}},"source":["save_folder = \"./data\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4X-QJOkWH1h7","colab_type":"text"},"source":["Following both datasets are saved. Please don't save other data from here. We are only using the two files as described above."]},{"cell_type":"code","metadata":{"id":"fX4-qKr8N9Au","colab_type":"code","colab":{}},"source":["def save_data(df: pd.DataFrame, filename: str):\n","  \"\"\"\n","  Saves data to the pickle file.\n","  Sample call:\n","  save_data(items, \"items.pk\")\n","  \"\"\"\n","  filename = f\"{save_folder}/{filename}\"\n","  df.to_pickle(filename)\n","\n","\n","save_data(items, 'static_features_final.pk' if make_final_submission else 'static_features.pk')\n","save_data(dynamic_feature, 'dynamic_features_final.pk' if make_final_submission else 'dynamic_features.pk')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jIKF7EI76YQj","colab_type":"text"},"source":["# FINAL DATA\n","These are the finalized dataframes"]},{"cell_type":"markdown","metadata":{"id":"jn0P9WXk6fDl","colab_type":"text"},"source":["## Static Features"]},{"cell_type":"code","metadata":{"id":"5SEhwR7O__0b","colab_type":"code","colab":{}},"source":["items"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OzxiJuuSIyMM","colab_type":"text"},"source":["## Dynamic Features"]},{"cell_type":"code","metadata":{"id":"ZMcyw7ufJ7wq","colab_type":"code","colab":{}},"source":["dynamic_feature"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_8A5c3jiMTUD","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}